
\documentclass[journal]{IEEEtran}
\usepackage{graphicx}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
\else
\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{A Path Planner for Unmanned Surface Vehicles: A Survey}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Riccardo Polvara% <-this % stops a space
\thanks{Riccardo Polvara is a PhD student with the Marine Science and Engineering School, University of Plymouth, Plymouth, England.
      E-mail: {\tt riccardo.polvara@plymouth.ac.uk}}}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
Developing a robust obstacle avoidance module is a foundamental step towards fully autonomous USVs. Until now, most of them move in the sea following way points paths,
 usually GPS-based, totally unaware about possible collisions against rocks, other vessels or also divers. In this paper, the actual state of the art regarding obstacle avoidance
 and the plan of a safe path between a starting point and a goal is summarized.
\end{abstract}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Path planner, USV, control, obstacle avoidance.
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle



\section{Introduction}
Marine robots represent one of the three big families in which mobile robotics could be divided, together with terrain and aerial robots. This kind of vehicles can be also distinguished in Unmanned Surface Vehicles (USVs) or Unmanned Underwater vehicles (UUVs) based on the fact they operate at the same level of the sea or under it.
The interest toward them and the advantages they can provide started in the last century, with the development of the COMOX torpedo concept by Canadians in 1944 as a pre-Normandy invasion USV designed to lay smoke during the invasion \cite{Bertram2008}.\\
\indent Sponsored primarly by the US Navy, multiple platforms were developed and deployed in the late 1990s with the scope of reconnaissance and surveillance missions. Between these, an example is given by the Owl MK II, a Jet Ski chassis equipped with a low-profile hull for increased stealth and payload capability, a sonar and a video camera. The military community has expressed strong interest in the use of USVs for a variety of roles, including force protection, surveillance, min warfare, anti-submarine warfarem riverine operations and special forces operations. This interest has been focused primarly on the development of the Spartan USV by the US Space and Naval Warfare System Center in San Diego since 2003. \\
\indent Multiple unmanned marine vehicles have been built also outside the USA: in Japan, for example, Yamaha developed the Unmanned Marine Vehicle High-Speed UMV-H and the Unmanned marine Vehicle Ocean type UMV-O, involved in bio-geo-chemical monitoring. Other examples are the Canadian Barracuda, the Dolphin MK II, the Seal USV and the SARPAL AMV, all developed by the International Submarine Engineering Ltd (ISE),the Stingray used by the Israeli navy, the Delfim and Caravela developed by the Portuguese Dynamical Systems and Ocean Robotics lab, and finally the Springer developed by the University of Plymouth.\\
\indent Most of the vessel cited before are dual-purpose vehicles, able to function in the conventional manned mode or in the unmanned one. Sometimes the installation of remoting kit allows the craft to be operated in fully manual mode, in autopilot-augmented mode or in remote-contrl mode. In this way the ship not only retains full manual capability but that capability is augmented and extended in an affordable and low-risk manner.\\
\indent To navigate in a fully autonomous way, marine vehicles require the presence of an \textit{obstacle avoidance module} able to move the vessel from the actual track to another one if an immediate collision is expected, and then take it back on the previous one towards the goal pose. As it usually happens with terrain robots, a path planner should be implemented: often it is distinguinshed in \textit{global path planner} (GPP) and \textit{local path planner} (LPP). The goal of GPP is to find a safe path connecting the starting position, the actual pose of the robot, and the final one, called \textit{goal pose}. In literature sometimes it is also called \textit{deliberative path planner}. Otherwise, the LPP has to react to immediate collision against obstacles unexpected or not considered by the GPP, moving the autonomous vehicles far from the preplanned path in order to avoid the moving obstacle; for this reason it is also called \textit{reactive path planner}.\\
\indent The structure of the paper is divided as follows: in Section \ref{obs_det} I illustrate how to perceive the environment surronding the autonomous vessel and detect static and moving obstacles with the most used computer vision techniques; in Sect. \ref{path_planner} I discuss more in details the necessity of having a robust path planner, therefore in subsections \ref{gpp} and \ref{lpp} I illustrate how a global and a local path planners, respectively, could be implemented. Finally, in Section \ref{conclusion} I propose a new combined path planner based on A* and other techniques presented in the previous sections.


\section{Obstacle Detection} \label{obs_det}
\indent To perfectly avoid the obstacles moving along the path of the autonomous vessel, an highly accurated world model is required. In order to obtain it, different sensors can be combined and data coming from them are usually fused in a 2D or 3D representation.\\
\indent In \cite{Almeida2009} the authors suggest to use an ARP radar sensor to identify moving obstacles and shores, and classify targets in terms of collision threat. They identify a set of perimeters around the USV in order to decide appropriate measures: \textit{irrilevant} perimeter(3km), \textit{safe} perimeter(500m), \textit{warning} perimeter(250m) and \textit{prohibition} one(50m). Based on the \textit{Closest Point of Approach} (CPA), defined as the estimated distance between the USV and the detected object at the time in which such distant is minimal, they classify targets as \textit{No Threat} (CPA outside Irrilevant perimeter), \textit{Low Threat} (CPA crosses the Irrilevenat perimeter but not the Safe one), \textit{Potential Threat} (CPA crosses the safe perimeter but not the Prohibited one) and \textit{Dangerous} (CPA inside the Prohibition perimeter).\\
\indent Low-Cost radars are also used in the work of Schuster \cite{Schuster2014}. The aim of this work is to propose an on-board collision avoidance approach for those situations in which the Automatic Identification System (AIS) is unavailable, and therefore the position, course, speed and dimensions of other vessels have to be estimated by radar measurements.\\
\indent In order to detect objects, data coming from radars need to be image preprocessing in the following way:
\begin{itemize}
\item Ego Motion Compensation: the azimuth of each scan in respect to the corresponging vessel's heading is calculated to compensate the vessel's yaw rate;
\item Occupancy Likelihood Determination: assuming that the probability \textit{p} of a cell being reported as occupied is independent and constant, the occupancy likelihood is binomially distributed;
\item Connected Component Labeling: a cell is assumed to contain a target if the occupancy probability is greater than 0.5; since a target usually extends of several hundred cells, adjacent, occupied cells are grouped using connected component labeling and each group is considered to be a target of elliptical shape.
\end{itemize}
The extracted target positions are very noisy, thus strong low pass filtering is required to obtain an object's true position, heading and velocity. To this scope, an \textit{Interactive Multiple Model} (IMM) filter is chosen: it runs several models in parallel and, based on the estimate of each model and the current measurement, a likelihood for each model to reflect the true motion state is determined. The output of the filter is a weighted sum of all model estimates and it is used by the collisions avoidance algorithm to predict the movement of the other vessels.
\indent Other approaches use monocular and stereo vision methods for recording the presence of obstacles in proximity (30 to 100 meters) of the vessel. An example is offered by the work of Wang \textit{et al.} \cite{Wang2011,Wang2012} in which two cameras are mounted parallel about 1.5 meters apart on a metal bar: the image from the camera on the left is initially used to perform monocular obstacle detection, and then the stero approach is applied to process the image from both cameras to compute the 3D detection results. The monocular technique is composed as follows:
\begin{itemize}
\item Horizon detection module: it allows to distinguish the sea surface for obstacle detection; it is realized using pixel profile analysis and RANSAC method to perform line fitting and extract the horizon;
\item Saliency detection module: as expressed in \cite{Achanta2009}, an image mask is built and the Euclidean distance between the color pixel vector in the gaussian blurred image and the average vector for the original color image is calculated; the detected salients will be given in the form of bounding boxes and are taken as the potential interested obstacles;
\item Harris corner extraction and tracking module: using the work proposed in \cite{Harris1988} and \cite{Bouguet1999}, motion evaluation is possible to distinguish surface obstacles from the potentials suggested by the saliency detection;
\item Obstacle detection module: the data coming from the previous modules are combined to generate the final results; since it is pratical to measure the dynamics of a potential object to verify its validity as an obstacle, a tracked feature with long lifespan in the salient bounding box is labelled as of high priority and this link the obstacles in consecutive frames.
\end{itemize}
\indent The stereo correspondence phase could be divided in three: an initial phase in which both cameras are calibrated so the results are used for stereo image undistortion and 3D reconstruction, an intermediate phase in which \textit{epipolar constraint} reduces the 2D search for obstacle correspondence, and then a \textit{stereo matching} phase in which the normalized cross correlation template matching method is adopted. Here, the bounding box of obstacled by monoculat obstacle detection in the left image is considered as the template window while the search is conducted along its epipolat line in the right image. In the end, a Kalman Filter is applied on the horizontal disparity in order to eliminate the stereo matching error and improve the range estimation accuracy.\\
\indent Another work similar to the previous one but that use only monular vision is the one proposed by Azzaby \textit{et al.} in \cite{Azzabi}. As before, the authors detect the horizon first and then the obstacles in the scene; last, they estimate the distance between the USV and objects, knowing the relative angle of each object and the horizion line. The algorithm developed for horizon detection can be summerized as follows:
\begin{itemize}
\item Grey level transformation: applied to the input image to reduce image time processing (despite of a little loss of information);
\item Sobel operator: it performs a 2D spatial gradient measurement on the image to emphasizes regions of high spatial frequency that correspond to edges;
\item Hough transform: a line passing through two pixels in the image domain must lie on the intersection of two lines in Hough domain;
\item Line election: it chooses the couple (\textit{a,b}) for which the line passes through the maximum of points and then traces this line that will be probably the horizon line.
\end{itemize}
\indent After this, the next step is detect objects in motion using optical flow estimation: given an input stream, the velocity of each pixel is firstly calculated; if a pixel has a velocity higher than a threshold value and is under the horizon, it is marked as moving and tracked and in the end the optical flow vector of each pixel is plotted in the frame.
\indent The output of the previous module is therefore used to estimate the distance from the USV to obstacles using a geometrical interpretation of the image. As result, the distance to the object, its width and its bearing angle is computed.

\section{Path Planner} \label{path_planner}

  \subsection{Global Path Planner} \label{gpp}

  \subsection{Local Path Planner} \label{lpp}




\section{Conclusion} \label{conclusion}



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliography{bibliography}
\bibliographystyle{plain}




% that's all folks
\end{document}
